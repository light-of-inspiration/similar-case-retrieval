import os
import json
import sys
import time
import gc
import torch
import numpy as np
from tqdm import tqdm
from pathlib import Path
import warnings

warnings.filterwarnings("ignore")

from processing import *
from jurex_qwen import *


# === è®¾å¤‡é…ç½® ===
def setup_torch_device():
    """è®¾ç½®PyTorch CUDAè®¾å¤‡"""
    print("=" * 60)
    print("PyTorch CUDAè®¾å¤‡è®¾ç½®")
    print("=" * 60)

    if torch.cuda.is_available():
        # é€‰æ‹©GPUè®¾å¤‡
        device = torch.device("cuda:0")  # æ˜ç¡®æŒ‡å®šç¬¬ä¸€ä¸ªGPU

        # è·å–è®¾å¤‡ä¿¡æ¯
        print(f"âœ… CUDAå¯ç”¨ï¼Œä½¿ç”¨è®¾å¤‡: {device}")
        print(f"   GPUåç§°: {torch.cuda.get_device_name(0)}")
        print(f"   GPUæ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024 ** 3:.2f} GB")
        print(f"   CUDAç‰ˆæœ¬: {torch.version.cuda}")
        print(f"   PyTorchç‰ˆæœ¬: {torch.__version__}")

        # è®¾ç½®PyTorchä¼˜åŒ–å‚æ•°
        torch.backends.cudnn.benchmark = True  # åŠ é€Ÿå·ç§¯è¿ç®—
        torch.backends.cuda.matmul.allow_tf32 = True  # å…è®¸TF32ï¼ŒåŠ é€ŸçŸ©é˜µä¹˜æ³•
        torch.backends.cudnn.allow_tf32 = True  # å…è®¸TF32

        # è®¾ç½®è®¾å¤‡ä¸ºå½“å‰è®¾å¤‡
        torch.cuda.set_device(device)

        # éªŒè¯è®¾å¤‡
        print(f"   å½“å‰è®¾å¤‡: {torch.cuda.current_device()}")
        print(f"   è®¾å¤‡æ•°é‡: {torch.cuda.device_count()}")

        return device
    else:
        print("âŒ CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPU")
        return torch.device("cpu")


# åˆå§‹åŒ–è®¾å¤‡
device = setup_torch_device()


# === llama_cppé›†æˆtorchè®¾å¤‡ ===
class TorchEnhancedLlama:
    """å¢å¼ºç‰ˆLlamaæ¨¡å‹ï¼Œé›†æˆPyTorchè®¾å¤‡ç®¡ç†"""

    def __init__(self, model_path, device=device):
        self.device = device
        self.model_path = model_path

        # å¯¼å…¥llama_cpp
        try:
            from llama_cpp import Llama
            self.Llama = Llama
        except ImportError:
            print("âŒ è¯·å®‰è£…llama-cpp-python: pip install llama-cpp-python")
            sys.exit(1)

        # åŠ è½½æ¨¡å‹
        self.model = self._load_model()

        # åˆ›å»ºCUDAå¼ é‡ç”¨äºç›‘æ§
        self._init_cuda_monitor()

    def _load_model(self):
        """åŠ è½½æ¨¡å‹å¹¶é…ç½®GPUå‚æ•°"""
        print(f"\nğŸš€ åŠ è½½æ¨¡å‹: {os.path.basename(self.model_path)}")

        # æ ¹æ®è®¾å¤‡ç±»å‹é…ç½®å‚æ•°
        if self.device.type == "cuda":
            n_gpu_layers = -1  # æ‰€æœ‰å±‚éƒ½åœ¨GPU
            n_batch = 2048  # å¢åŠ æ‰¹å¤„ç†å¤§å°
            n_threads = 8  # CPUçº¿ç¨‹æ•°
        else:
            n_gpu_layers = 0
            n_batch = 512
            n_threads = 16

        config = {
            'model_path': self.model_path,
            'n_ctx': 32768,
            'n_gpu_layers': n_gpu_layers,
            'n_batch': n_batch,
            'n_threads': n_threads,
            'offload_kqv': True,
            'flash_attn': True,
            'use_mmap': True,
            'use_mlock': False,
            'verbose': False
        }

        print(f"   é…ç½®: GPUå±‚æ•°={n_gpu_layers}, æ‰¹å¤§å°={n_batch}, çº¿ç¨‹æ•°={n_threads}")

        try:
            model = self.Llama(**config)
            print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ!")
            return model
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            # å°è¯•ç®€åŒ–é…ç½®
            print("å°è¯•ç®€åŒ–é…ç½®...")
            try:
                model = self.Llama(
                    model_path=self.model_path,
                    n_ctx=8192,
                    n_gpu_layers=n_gpu_layers,
                    verbose=False
                )
                print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼ˆç®€åŒ–é…ç½®ï¼‰!")
                return model
            except Exception as e2:
                print(f"âŒ ç®€åŒ–é…ç½®ä¹Ÿå¤±è´¥: {e2}")
                raise

    def _init_cuda_monitor(self):
        """åˆå§‹åŒ–CUDAç›‘æ§å¼ é‡"""
        if self.device.type == "cuda":
            # åˆ›å»ºä¸€äº›CUDAå¼ é‡ç”¨äºç›‘æ§GPUä½¿ç”¨
            self.monitor_tensor = torch.zeros(1000, 1000, device=self.device)
            print(f"âœ… CUDAç›‘æ§å¼ é‡åˆ›å»ºåœ¨: {self.monitor_tensor.device}")

    def create_completion(self, prompt, **kwargs):
        """åˆ›å»ºå®Œæˆï¼Œé›†æˆGPUç›‘æ§"""
        # åœ¨GPUä¸Šæ‰§è¡Œæ¨ç†
        if self.device.type == "cuda":
            # ç¡®ä¿ç›‘æ§GPUä½¿ç”¨
            self._log_gpu_memory("æ¨ç†å‰")

        # è°ƒç”¨åŸå§‹æ¨¡å‹
        result = self.model.create_completion(prompt, **kwargs)

        if self.device.type == "cuda":
            self._log_gpu_memory("æ¨ç†å")

        return result

    def _log_gpu_memory(self, stage):
        """è®°å½•GPUå†…å­˜ä½¿ç”¨"""
        if self.device.type == "cuda":
            allocated = torch.cuda.memory_allocated(self.device) / 1024 ** 3
            reserved = torch.cuda.memory_reserved(self.device) / 1024 ** 3

            print(f"   [{stage}] GPUå†…å­˜: åˆ†é…={allocated:.2f}GB, ä¿ç•™={reserved:.2f}GB")


# === GPUåŠ é€Ÿæ•°æ®å¤„ç† ===
class CUDADataProcessor:
    """CUDAåŠ é€Ÿçš„æ•°æ®å¤„ç†å™¨"""

    def __init__(self, device):
        self.device = device
        self.data_cache = {}  # ç¼“å­˜æ•°æ®åˆ°GPU

    def process_text_batch(self, texts, max_length=512):
        """æ‰¹é‡å¤„ç†æ–‡æœ¬åˆ°CUDAå¼ é‡"""
        if not texts:
            return None

        # åˆ›å»ºæ‰¹æ¬¡
        batch_size = len(texts)

        # å°†æ–‡æœ¬ç¼–ç ä¸ºæ•°å­—ï¼ˆç®€åŒ–ç¤ºä¾‹ï¼Œå®é™…éœ€è¦tokenizerï¼‰
        encoded_texts = [self._simple_encode(text, max_length) for text in texts]

        # è½¬æ¢ä¸ºPyTorchå¼ é‡å¹¶ç§»åŠ¨åˆ°CUDA
        tensor_batch = torch.tensor(encoded_texts, dtype=torch.long)

        # ä½¿ç”¨torch.to(device)ç§»åŠ¨åˆ°GPU
        tensor_batch = tensor_batch.to(self.device)

        print(f"âœ… æ•°æ®æ‰¹æ¬¡å·²ç§»åŠ¨åˆ° {self.device}: {tensor_batch.shape}")

        return tensor_batch

    def _simple_encode(self, text, max_length):
        """ç®€å•çš„æ–‡æœ¬ç¼–ç ï¼ˆå®é™…åº”ç”¨åº”ä½¿ç”¨åˆé€‚çš„tokenizerï¼‰"""
        # ç®€åŒ–çš„ç¼–ç ï¼šå°†å­—ç¬¦è½¬æ¢ä¸ºASCIIç 
        encoded = [ord(c) for c in text[:max_length]]

        # å¡«å……æˆ–æˆªæ–­
        if len(encoded) < max_length:
            encoded += [0] * (max_length - len(encoded))
        else:
            encoded = encoded[:max_length]

        return encoded

    def cache_to_gpu(self, key, data):
        """ç¼“å­˜æ•°æ®åˆ°GPU"""
        if isinstance(data, (list, np.ndarray)):
            # è½¬æ¢ä¸ºPyTorchå¼ é‡
            tensor = torch.tensor(data)
            # ä½¿ç”¨torch.to(device)ç§»åŠ¨åˆ°GPU
            tensor = tensor.to(self.device)
            self.data_cache[key] = tensor
            print(f"âœ… æ•°æ®å·²ç¼“å­˜åˆ°GPU: {key} -> {tensor.shape}")
            return tensor
        return data


# === æ‰¹é‡æ¨ç†å¤„ç†å™¨ ===
class CUDABatchProcessor:
    """CUDAåŠ é€Ÿçš„æ‰¹é‡å¤„ç†å™¨"""

    def __init__(self, llama_model, device, batch_size=4):
        self.llama_model = llama_model
        self.device = device
        self.batch_size = batch_size

        # åˆ›å»ºCUDAæ•°æ®å¤„ç†å™¨
        self.data_processor = CUDADataProcessor(device)

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'processed': 0,
            'batches': 0,
            'gpu_time': 0.0,
            'total_tokens': 0
        }

    def batch_generate(self, prompts, max_tokens=512, temperature=0.7):
        """æ‰¹é‡ç”Ÿæˆï¼Œä½¿ç”¨CUDAåŠ é€Ÿ"""
        all_responses = []

        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()

        # åˆ†æ‰¹å¤„ç†
        for i in range(0, len(prompts), self.batch_size):
            batch_prompts = prompts[i:i + self.batch_size]
            self.stats['batches'] += 1

            # åœ¨GPUä¸Šé¢„å¤„ç†æ•°æ®
            with torch.cuda.device(self.device):
                # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ æ•°æ®é¢„å¤„ç†æ­¥éª¤
                # ä¾‹å¦‚ï¼šå°†æ–‡æœ¬è½¬æ¢ä¸ºCUDAå¼ é‡

                # æ‰§è¡Œæ¨ç†
                batch_start = time.time()
                batch_responses = self._process_batch(batch_prompts, max_tokens, temperature)
                batch_time = time.time() - batch_start

                self.stats['gpu_time'] += batch_time
                self.stats['total_tokens'] += sum(len(p) for p in batch_prompts)

                all_responses.extend(batch_responses)

                # ç›‘æ§GPUä½¿ç”¨
                self._monitor_gpu(f"æ‰¹æ¬¡ {i // self.batch_size + 1}")

        total_time = time.time() - start_time
        print(f"ğŸ“Š æ‰¹æ¬¡å¤„ç†å®Œæˆ: {self.stats['batches']}æ‰¹æ¬¡, "
              f"GPUæ—¶é—´: {self.stats['gpu_time']:.2f}ç§’, "
              f"æ€»æ—¶é—´: {total_time:.2f}ç§’")

        return all_responses

    def _process_batch(self, prompts, max_tokens, temperature):
        """å¤„ç†å•ä¸ªæ‰¹æ¬¡"""
        responses = []

        for prompt in prompts:
            try:
                # ä½¿ç”¨llamaæ¨¡å‹ç”Ÿæˆ
                response = self.llama_model.create_completion(
                    prompt=prompt,
                    max_tokens=max_tokens,
                    temperature=temperature,
                    top_p=0.9,
                    repeat_penalty=1.1,
                    stop=["<|endoftext|>", "</s>", "###"]
                )

                if response and 'choices' in response:
                    text = response['choices'][0]['text']
                    responses.append(text)
                else:
                    responses.append("")

            except Exception as e:
                print(f"âŒ æ¨ç†å¤±è´¥: {e}")
                responses.append("")

        return responses

    def _monitor_gpu(self, label=""):
        """ç›‘æ§GPUä½¿ç”¨æƒ…å†µ"""
        if self.device.type == "cuda":
            allocated = torch.cuda.memory_allocated(self.device) / 1024 ** 3
            reserved = torch.cuda.memory_reserved(self.device) / 1024 ** 3

            # è·å–GPUåˆ©ç”¨ç‡ï¼ˆéœ€è¦pynvmlï¼‰
            try:
                import pynvml
                pynvml.nvmlInit()
                handle = pynvml.nvmlDeviceGetHandleByIndex(self.device.index or 0)
                util = pynvml.nvmlDeviceGetUtilizationRates(handle)
                print(f"   {label} GPUä½¿ç”¨: {allocated:.2f}GB, åˆ©ç”¨ç‡: {util.gpu}%")
            except:
                print(f"   {label} GPUå†…å­˜: {allocated:.2f}GB")

            # å¦‚æœå†…å­˜ä½¿ç”¨è¿‡é«˜ï¼Œæ¸…ç†ç¼“å­˜
            if allocated > 18:  # è¶…è¿‡18GB
                torch.cuda.empty_cache()


# === å€™é€‰æ•°æ®å¤„ç† ===
class CandidateProcessor:
    """å€™é€‰æ•°æ®å¤„ç†å™¨ï¼ŒåŒ…å«deså±æ€§æ£€æŸ¥"""

    def __init__(self, batch_processor):
        self.batch_processor = batch_processor
        self.device = batch_processor.device

        # å¯¼å…¥æ•°æ®åŠ è½½å‡½æ•°
        try:
            from processing import getCandidateDict, getCandidateCrimes
            self.getCandidateDict = getCandidateDict
            self.getCandidateCrimes = getCandidateCrimes
        except ImportError:
            print("âŒ æ— æ³•å¯¼å…¥processingæ¨¡å—")
            raise

    def check_des_exists(self, c_dict):
        """æ£€æŸ¥deså±æ€§æ˜¯å¦å·²å­˜åœ¨ä¸”ä¸ä¸ºç©º"""
        if not c_dict:
            return True

        # æ£€æŸ¥deså­—æ®µæ˜¯å¦å­˜åœ¨
        if 'des' not in c_dict:
            return False

        des_value = c_dict['des']

        # æ£€æŸ¥desæ˜¯å¦ä¸ºç©º
        if not des_value:
            return False

        # æ£€æŸ¥desæ˜¯å¦ä¸ºæœ‰æ•ˆå­—å…¸ä¸”æœ‰å†…å®¹
        if isinstance(des_value, dict) and len(des_value) > 0:
            return True

        # æ£€æŸ¥æ˜¯å¦ä¸ºå…¶ä»–éç©ºå€¼
        if des_value:
            return True

        return False

    def generate_prompt(self, c_dict, crime):
        """ç”Ÿæˆæç¤ºè¯"""
        text = c_dict.get('ajjbqk', '') + c_dict.get('cpfxgc', '')

        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ³•å¾‹åˆ†æä¸“å®¶ã€‚è¯·åˆ†æä»¥ä¸‹æ¡ˆä¾‹æ˜¯å¦æ„æˆã€{crime}ã€‘ç½ªåï¼š

ã€æ¡ˆä»¶äº‹å®ã€‘
{text}

è¯·ä»ä»¥ä¸‹ä¸‰ä¸ªç»´åº¦è¿›è¡Œåˆ†æï¼š
1. ç½ªååˆ†æï¼šæ˜¯å¦æ„æˆè¯¥ç½ªåï¼Œä¸ºä»€ä¹ˆ
2. æ„æˆè¦ä»¶ï¼šä¸»ä½“ã€å®¢ä½“ã€ä¸»è§‚æ–¹é¢ã€å®¢è§‚æ–¹é¢
3. é‡åˆ‘æƒ…èŠ‚ï¼šä»é‡ã€ä»è½»ã€å‡è½»æˆ–å…é™¤å¤„ç½šæƒ…èŠ‚

è¯·ç”¨JSONæ ¼å¼è¾“å‡ºåˆ†æç»“æœï¼š"""

        return prompt

    def process_candidate(self, ridx, cid):
        """å¤„ç†å•ä¸ªå€™é€‰"""
        try:
            # åŠ è½½å€™é€‰æ•°æ®
            c_dict = self.getCandidateDict(ridx, cid)

            # æ£€æŸ¥deså±æ€§
            if self.check_des_exists(c_dict):
                return None, "å·²å­˜åœ¨deså±æ€§"

            # è·å–çŠ¯ç½ªåˆ—è¡¨
            crime_list = self.getCandidateCrimes(ridx, cid)
            if not crime_list:
                c_dict['des'] = {}
                return c_dict, "æ— çŠ¯ç½ªåˆ—è¡¨"

            # ä¸ºæ¯ä¸ªçŠ¯ç½ªç”Ÿæˆæç¤ºè¯
            prompts = []
            crimes = []

            for crime in crime_list:
                prompt = self.generate_prompt(c_dict, crime)
                prompts.append(prompt)
                crimes.append(crime)

            # æ‰¹é‡æ¨ç†ï¼ˆä½¿ç”¨CUDAåŠ é€Ÿï¼‰
            print(f"  æ­£åœ¨å¤„ç† {len(prompts)} ä¸ªæç¤ºè¯...")
            responses = self.batch_processor.batch_generate(prompts)

            # è§£æå“åº”å¹¶æ„å»ºdeså­—å…¸
            c_dict['des'] = {}
            for crime, response in zip(crimes, responses):
                # è§£æå“åº”ï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…éœ€è¦æ›´å¤æ‚çš„è§£æï¼‰
                crime_desc = self._parse_response(response)
                c_dict['des'][crime] = crime_desc

            return c_dict, "æˆåŠŸ"

        except Exception as e:
            print(f"âŒ å¤„ç†å€™é€‰ {ridx}/{cid} å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None, str(e)

    def _parse_response(self, response):
        """è§£æå“åº”"""
        # ç®€åŒ–çš„è§£æï¼Œå®é™…åº”æ ¹æ®æ¨¡å‹è¾“å‡ºæ ¼å¼è°ƒæ•´
        if not response:
            return {"error": "ç©ºå“åº”"}

        # å°è¯•æå–JSON
        import re
        json_match = re.search(r'\{.*\}', response, re.DOTALL)

        if json_match:
            try:
                return json.loads(json_match.group(0))
            except:
                pass

        # å¦‚æœä¸æ˜¯JSONï¼Œè¿”å›åŸå§‹æ–‡æœ¬
        return {"analysis": response[:500]}  # é™åˆ¶é•¿åº¦


# === ä¸»ç¨‹åº ===
def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "=" * 70)
    print("æœ¬åœ°GGUFæ¨¡å‹ + PyTorch CUDAåŠ é€Ÿç³»ç»Ÿ")
    print("=" * 70)

    # 1. è®¾ç½®CUDAè®¾å¤‡
    device = setup_torch_device()

    # 2. æŸ¥æ‰¾æœ¬åœ°æ¨¡å‹
    model_dir = Path("E:/Data/models/")
    gguf_models = list(model_dir.glob("*.gguf"))

    if not gguf_models:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°GGUFæ¨¡å‹æ–‡ä»¶")
        return

    print(f"\nğŸ“‚ æ‰¾åˆ° {len(gguf_models)} ä¸ªæœ¬åœ°æ¨¡å‹:")
    for i, model_path in enumerate(gguf_models):
        print(f"  [{i + 1}] {model_path.name} ({model_path.stat().st_size / 1024 ** 3:.2f} GB)")

    # é€‰æ‹©ç¬¬ä¸€ä¸ªæ¨¡å‹
    selected_model = gguf_models[0]
    print(f"\nâœ… é€‰æ‹©æ¨¡å‹: {selected_model.name}")

    # 3. åŠ è½½æ¨¡å‹
    try:
        llama_model = TorchEnhancedLlama(str(selected_model), device)
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return

    # 4. åˆ›å»ºCUDAåŠ é€Ÿçš„æ‰¹å¤„ç†å™¨
    batch_processor = CUDABatchProcessor(llama_model, device, batch_size=4)

    # 5. åˆ›å»ºå€™é€‰å¤„ç†å™¨
    candidate_processor = CandidateProcessor(batch_processor)

    # 6. å¤„ç†æ•°æ®
    ROOT_PATH = Path(r"E:/Py_Dev/IceBerg/data/candidates")

    if not ROOT_PATH.exists():
        print(f"âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {ROOT_PATH}")
        return

    # è·å–æ‰€æœ‰ridxç›®å½•
    all_ridx = [d for d in ROOT_PATH.iterdir() if d.is_dir()]

    print(f"\nğŸ“ æ‰¾åˆ° {len(all_ridx)} ä¸ªå€™é€‰ç›®å½•")

    total_processed = 0
    total_skipped = 0
    total_errors = 0

    # å¤„ç†æ¯ä¸ªç›®å½•
    for ridx_dir in all_ridx:
        ridx = ridx_dir.name
        print(f"\nğŸ“‚ å¤„ç†ç›®å½•: {ridx}")

        # è·å–æ‰€æœ‰JSONæ–‡ä»¶
        json_files = list(ridx_dir.glob("*.json"))

        if not json_files:
            print(f"   æ²¡æœ‰JSONæ–‡ä»¶")
            continue

        print(f"   æ‰¾åˆ° {len(json_files)} ä¸ªå€™é€‰æ–‡ä»¶")

        # å¤„ç†æ¯ä¸ªæ–‡ä»¶
        for json_file in tqdm(json_files, desc=f"å¤„ç† {ridx}"):
            cid = json_file.stem  # å»æ‰æ‰©å±•å

            # å¤„ç†å€™é€‰
            result, status = candidate_processor.process_candidate(ridx, cid)

            if status == "å·²å­˜åœ¨deså±æ€§":
                total_skipped += 1
                continue
            elif status == "æˆåŠŸ":
                # ä¿å­˜ç»“æœ
                try:
                    with open(json_file, 'w', encoding='utf-8') as f:
                        json.dump(result, f, ensure_ascii=False, indent=2)
                    total_processed += 1
                except Exception as e:
                    print(f"âŒ ä¿å­˜æ–‡ä»¶å¤±è´¥ {json_file}: {e}")
                    total_errors += 1
            else:
                total_errors += 1

            # å®šæœŸæ¸…ç†GPUç¼“å­˜
            if total_processed % 10 == 0 and device.type == "cuda":
                torch.cuda.empty_cache()
                gc.collect()

    # 7. æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print("\n" + "=" * 70)
    print("ğŸ‰ å¤„ç†å®Œæˆ!")
    print("=" * 70)
    print(f"   æ€»è®¡å¤„ç†: {total_processed}")
    print(f"   æ€»è®¡è·³è¿‡: {total_skipped}")
    print(f"   æ€»è®¡é”™è¯¯: {total_errors}")

    # 8. æœ€ç»ˆGPUçŠ¶æ€
    if device.type == "cuda":
        print("\nğŸ“Š æœ€ç»ˆGPUçŠ¶æ€:")
        print(f"   å·²åˆ†é…å†…å­˜: {torch.cuda.memory_allocated(device) / 1024 ** 3:.2f} GB")
        print(f"   ä¿ç•™å†…å­˜: {torch.cuda.memory_reserved(device) / 1024 ** 3:.2f} GB")

        # æ¸…ç†
        torch.cuda.empty_cache()
        print("âœ… GPUç¼“å­˜å·²æ¸…ç†")


# === è„šæœ¬å…¥å£ ===
if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­ï¼Œæ­£åœ¨æ¸…ç†...")
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        print("âœ… æ¸…ç†å®Œæˆ")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºå¼‚å¸¸: {e}")
        import traceback

        traceback.print_exc()

        # ç¡®ä¿æ¸…ç†GPUå†…å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            gc.collect()